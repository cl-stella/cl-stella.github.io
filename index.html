<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv='cache-control' content='no-cache'> 
  <meta http-equiv='expires' content='0'> 
  <meta http-equiv='pragma' content='no-cache'>
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="STELLA">
  <meta property="og:title" content="STELLA (2024)"/>
  <meta property="og:description" content="STELLA: Continual Audio-Video Pre-training with Spatio-Temporal Localized Alignment"/>
  <meta property="og:url" content="https://cl-stella.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/shooting-star-1.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="STELLA (2024)">
  <meta name="twitter:description" content="STELLA: Continual Audio-Video Pre-training with Spatio-Temporal Localized Alignment">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/shooting-star-1.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="multimodal learning, continual learning, lifelong learning, task-free, audio-video learning, audiovisual learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>STELLA: Continual Audio-Video Pre-training with Spatio-Temporal Localized Alignment</title>
  <link rel="icon" type="image/x-icon" href="static/images/shooting-star-1.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
      .left-align {
          text-align: left;
      }
  </style>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3 publication-title">STELLA: Continual Audio-Video Pre-training with Spatio-Temporal Localized Alignment</h1>
            <div class="is-size-5 publication-authors">

              <!-- Paper authors -->
              <span class="author-block">
                Jaewoo Lee<sup>*,1</sup>,</span>
                <span class="author-block", style="padding-left:30px">
                  <a href="https://jaehong31.github.io/" target="_blank">Jaehong Yoon</a><sup>* 2</sup></span>
                  <span class="author-block", style="padding-left:30px">
                    <a href="https://wonjae.kim/" target="_blank">Wonjae Kim</a><sup>3</sup>,</span>
                      <span class="author-block", style="padding-left:30px">
                        <a href="https://github.com/YunjiKim" target="_blank">Yunji Kim</a><sup>3</sup>,</span>
                        <span class="author-block", style="padding-left:30px">
                          <a href="http://www.sungjuhwang.com/" target="_blank">Sung Ju Hwang</a><sup>1,4</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1 </sup>KAIST, <sup>2 </sup>UNC Chapel Hill, <sup>3 </sup>NAVER AI Lab <sup>4 </sup>DeepAuto <br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

<!--                  <div class="column has-text-centered">-->
<!--                    <div class="publication-links">-->
<!--                      &lt;!&ndash; Arxiv PDF link &ndash;&gt;-->
<!--                      <span class="link-block">-->
<!--                        <a href="https://arxiv.org/abs/2310.08204" target="_blank"-->
<!--                        class="external-link button is-normal is-rounded is-dark">-->
<!--                        <span class="icon">-->
<!--                          <i class="fas fa-file-pdf"></i>-->
<!--                        </span>-->
<!--                        <span>Paper</span>-->
<!--                      </a>-->
<!--                    </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/G-JWLEE/STELLA_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

<!--                 ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2310.08204" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
  </section>

<!-- Paper abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Continuously learning a variety of audio-video semantics over time is crucial for audio-related reasoning tasks in our ever-evolving world.
            However, this is a nontrivial problem and poses two critical challenges: <i>sparse spatio-temporal correlation between audio-video pairs</i> and <i>multimodal correlation overwriting</i> that forgets audio-video relations.
            To tackle this problem, we propose a new continual audio-video pre-training method with two novel ideas:
            <i>(1) Localized Patch Importance Scoring</i>: we introduce a multimodal encoder to determine the importance score for each patch, emphasizing semantically intertwined audio-video patches.
            <i>(2) Replay-guided Correlation Assessment</i>: to reduce the corruption of previously learned audiovisual knowledge due to drift,
            we propose to assess the correlation of the current patches on the past steps to identify the patches exhibiting high correlations with the past steps.
            Based on the results from the two ideas, we perform probabilistic patch selection for effective continual audio-video pre-training.
            Experimental validation on multiple benchmarks shows that our method achieves a 3.69%p of relative performance gain in zero-shot retrieval tasks compared to strong continual learning baselines,
            while reducing memory consumption by ~45%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Motivation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Motivation</h2>

    </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Emerging new audio-video semantics </h2>

    <center><img src="static/images/motivation_1.png" alt="Teaser" width="75%"></center>

    <div class="content has-text-justified">
      Outdated pre-trained audio-video models struggle with understanding emerging new audio-video semantics.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Challenges in continual audio-video learning </h2>

    <center><img src="static/images/motivation_2.png" alt="Teaser" width="75%"></center>

    <div class="content has-text-justified">
      Learning audio-video data with continuously changing semantic categories is a nontrivial problem due to two critical challenges:
      <i>1) sparse spatio-temporal correlation between audio-video pairs,</i> and <i>2) multimodal correlation overwriting.</i>
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Challenge of multimodal correlation overwriting </h2>

    <center><img src="static/images/motivation_3.png" alt="Teaser" width="75%"></center>

    <div class="content has-text-justified">
      During continual pre-training, the model can encounter new semantics sharing key visual objects, making the model overwrite the previously learned audio information, resulting in forgetting.
    </div>
  </div>
  </div>

  </div>
</section>


<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Method</h2>

      <center><img src="static/images/concept_figure.png" alt="Teaser" width="100%"></center>

      <div class="content has-text-justified">
        Our method harnesses cross-modal attention maps from the AVM module to compute importance scores in order to identify highly correlated patches (<b>Localized Patch Importance Scoring</b>).
        Comparing the attention maps created by the current queries with those generated by past queries, we compute correlation scores of the current patches with the past data (<b>Replay-guided Correlation Assessment</b>).
        Finally, we perform a probabilistic patch selection, combining the importance scores and correlation scores to select patches for continual audio-video pre-training (<b>Multimodal Patch Selection for Continual Learning</b>).
      </div>

    </div>
  </div>
  </div>
</section>


<!-- Main results  -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Experiment Results</h2>

    </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Continual-VS, Continual-AS - Zero-shot audiovisual retrieval tasks </h2>

    <center><img src="static/images/result_1.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      We split VGGSound and AudioSet datasets into multiple tasks based on its high-level category information, naming each as <i>Continual-VS</i> and <i>Continual-AS</i>, respectively.
      The table shows results of audiovisual zero-shot retrieval task on the <i>Continual-VS</i> and <i>Continual-AS</i>. Our methods outperform strong baselines, especially in R1 score.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Downstream performance on various rehearsal memory sizes </h2>

    <center><img src="static/images/result_5.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      We explore the influence of rehearsal memory size on audiovisual zero-shot retrieval task performances.
      Our method consistently surpass other baselines, underscoring their effectiveness in adapting to diverse memory constraints.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">MRS-VTT audiovisual retrieval task</h2>

    <center><img src="static/images/result_4.png" alt="Teaser" width="80%"></center>

    <div class="content has-text-justified">
      We use the models continually pre-trained until the completion of the last task of <i>Continual-VS</i>, and conduct audiovisual retrieval experiments on the MST-VTT dataset.
      Our methods acquire transferable understanding audio-video relationships, leading to high performances in the task.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Sound source localization with the AVE dataset</h2>

    <center><img src="static/images/result_6.png" alt="Teaser" width="80%"></center>

    <div class="content has-text-justified">
      We perform a sound source localization task on the AVE dataset to evaluate the model's ability to detect sound sources within visual scenes.
      Compared to other baselines, our AVM module in <i>STELLA</i> stands out by precisely identifying the correct sound source.
    </div>
  </div>
  </div>

  </div>
</section>


<!-- Main results  -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Analysis</h2>

    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Efficiency analysis</h2>

    <center><img src="static/images/result_2.png" alt="Teaser" width="80%"></center>

    <div class="content has-text-justified">
      We measure GPU memory occupancy (GPU M.) in GB, estimated in a V100 with a batch size of 9. <i>STELLA</i> achieves an efficiency gain of <b>43.59%</b>.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Analysis on core sampling methods in <i>STELLA</i></h2>

    <center><img src="static/images/result_3.png" alt="Teaser" width="80%"></center>

    <div class="content has-text-justified">
      We experiment on the effect of our two core sampling methods: <i>(1) LPIS: Localized Patch Importance Scoring</i> and <i>(2) RCA: Replay-guided Correlation Assessment</i>.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Modality gap estimation</h2>

    <center><img src="static/images/result_7.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      During continual pre-training, we estimate the modality gap at the end of each task. In the context of continual audio-video pre-training,
      maintaining a large modality gap between the two modalities throughout tasks is desirable, as deviating from it suggests a departure from the optimal state.
    </div>
  </div>
  </div>



  </div>
</section>



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{lee2023lifelong,
      title={STELLA: Continual Audio-Video Pre-training with Spatio-Temporal Localized Alignment},
      author={Jaewoo Lee and Jaehong Yoon and Wonjae Kim and Yunji Kim and Sung Ju Hwang},
      year={2024},
      eprint={2310.08204},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
